
# Machine Learning

# 機械学習・ニューラルネットワーク

xxx

# ニューラルネットワークの構造

- 回帰（連続値）
- 分類（離散値）
- 重み（パラメータ）
- バイアス
- 入力層
- 中間層（隠れ層）
- 出力層
- 全結合層
- ハイパーパラメータ: モデルを学習する前にあらかじめ人の手で決められるパラメータのこと
- 目的変数
- 活性化関数: 非線形変換を行う関数
    - 中間層:
        - シグモイド関数: 勾配（微分）が消失する恐れがあり、今はあまり使われない。
        - 正規化線形関数（ReLU）: 0または0以上のパラメータを採用する。`h = f(u) = max(0, u)`
    - 出力層
        - ソフトマックス間奏: 分類で使われる。分類ごとの確率を出力し、分類ごとの確率を全て足すと1になる。
- 目的関数（損失関数）: 目標値と予測値の誤差を求める。回帰と分類で使う関数が異なることが多い。
    - 回帰
        - 平均二乗誤差
    - 分類
        - 交差エントロピー
- 逆伝播: パラメータを更新する
    - 最急降下法（SGD・確率的勾配降下法）: 初期値を檀ダム
    - 誤差逆伝播法・連鎖律
- accuracy: 正解率


# ニューラルネットワークの計算

`1.` `2.` を繰り返し、最終的に`3.`を求める

1. 線形変換: 数式の中心となるもの
2. 非線形変換: 線形変換を柔軟に対応させるもの
3. 目的関数（損失関数）の計算


